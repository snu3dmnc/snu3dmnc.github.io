
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate. ","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://localhost:1313/author/snu-rvl-lab/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/author/snu-rvl-lab/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.","tags":null,"title":"SNU RVL Lab","type":"authors"},{"authors":["SNU Visual \u0026 Geometric Intelligence Lab."],"categories":null,"content":"We present a targetless LiDAR-camera calibration method that jointly optimizes sensor poses and scene geometry from arbitrary scenes, without relying on traditional calibration targets such as checkerboards or spherical reflectors. Our approach leverages a 3D Gaussian-based scene representation. We first freeze reliable LiDAR points as anchors, then jointly optimize the poses and auxiliary Gaussian parameters in a fully differentiable manner using a photometric loss. This joint optimization significantly reduces sensor misalignment, resulting in higher rendering quality and consistently improved PSNR compared to the carefully calibrated poses provided in popular datasets. We validate our method through extensive experiments on two real-world autonomous driving datasets, KITTI-360 and Waymo, each featuring distinct sensor configurations. Additionally, we demonstrate the robustness of our approach using a custom LiDAR-camera setup, confirming strong performance across diverse hardware configurations.\nProject Website\n","date":1746057600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1746057600,"objectID":"3b51ea257a7562d1c2ee6e0b397201ba","permalink":"http://localhost:1313/post/25-05-01-targetlesscalib/","publishdate":"2025-05-01T00:00:00Z","relpermalink":"/post/25-05-01-targetlesscalib/","section":"post","summary":"We present a targetless LiDAR-camera calibration method that jointly optimizes sensor poses and scene geometry from arbitrary scenes, without relying on traditional calibration targets such as checkerboards or spherical reflectors. Our approach leverages a 3D Gaussian-based scene representation. We first freeze reliable LiDAR points as anchors, then jointly optimize the poses and auxiliary Gaussian parameters in a fully differentiable manner using a photometric loss. This joint optimization significantly reduces sensor misalignment, resulting in higher rendering quality and consistently improved PSNR compared to the carefully calibrated poses provided in popular datasets. We validate our method through extensive experiments on two real-world autonomous driving datasets, KITTI-360 and Waymo, each featuring distinct sensor configurations. Additionally, we demonstrate the robustness of our approach using a custom LiDAR-camera setup, confirming strong performance across diverse hardware configurations.\n","tags":null,"title":"Targetless LiDAR-Camera Calibration with Anchored 3D Gaussians","type":"post"},{"authors":["SNU Visual \u0026 Geometric Intelligence Lab."],"categories":null,"content":"High-Precision Outdoor 3D Color Mapping Using LiDAR and Camera Sensor Capture Systems.\nThis project was carried out to introduce the Haedong Advanced Engineering Hall, a new building at Seoul National University. This project page was created to capture the exterior of the Haedong Advanced Engineering Hall and visualize the reconstructed results. We generated a LiDAR prior map using a LiDAR SLAM-based method and estimated the camera poses through our point cloud-based Structure-from-Motion (SfM) technique. By accurately determining the camera positions, we reconstructed large-scale outdoor scenes in block units. This process allowed us to produce a high-quality, colorized point cloud and achieve high-fidelity rendered videos using 3D Gaussian Splatting. You can explore more detailed results on the webpage below. Subsequently, we built a system capable of processing both LiDAR and camera data for 3D neural modeling based on 3D Gaussian Splatting. To handle large-scale sensor data efficiently, we introduced a distributed processing technique called Trajectory Chunking.\n","date":1731369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1731369600,"objectID":"9710d0cf27ee801da257fc38464275b4","permalink":"http://localhost:1313/post/24-10-02-outdoor3d/","publishdate":"2024-11-12T00:00:00Z","relpermalink":"/post/24-10-02-outdoor3d/","section":"post","summary":"High-Precision Outdoor 3D Color Mapping Using LiDAR and Camera Sensor Capture Systems.\n","tags":null,"title":"3D Reconstruction of SNU’s Haedong Building","type":"post"},{"authors":["SNU 3D Vision Lab."],"categories":null,"content":"Downloads\nCapture with omnidirectional camera attached to a selfie stick\nFaster \u0026amp; easier capture than conventional camera (~1 min.) 5760 x 2880 resolution Estimate camera poses\nEvenly sample frames Convert equirectangular images to cubemap images Run SfM with perspective images (Worked better than run SfM directly on 360 images) ","date":1730592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730592000,"objectID":"d50f679d6d0984d3ec13bf845b769f1c","permalink":"http://localhost:1313/datasets/indoor3d/","publishdate":"2024-11-03T00:00:00Z","relpermalink":"/datasets/indoor3d/","section":"datasets","summary":"Indoor Insta360 Captured datasets","tags":[],"title":"Indoor Datasets","type":"datasets"},{"authors":["SNU Robot Vision Lab."],"categories":null,"content":"Downloads\nDatasets includes :\nOuster 32ch LiDAR 4 Ultra-wide Fisheye Camera Front-facing Intel D435 ","date":1730592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1730592000,"objectID":"c88497f4e906e8dbe76cba5f483af29e","permalink":"http://localhost:1313/datasets/navlidar/","publishdate":"2024-11-03T00:00:00Z","relpermalink":"/datasets/navlidar/","section":"datasets","summary":"3D Lidar Datasets (with Indoor and outdoor navigation)","tags":[],"title":"LiDAR Robot Driving Datasets","type":"datasets"},{"authors":["SNU Robust Perception and Mobile Robotics Lab."],"categories":null,"content":"“Development of SLAM Technology for Robots Adaptable to Temporal Changes”.\nSingle-session Static Mapping (Dynamic Object removal) Multi-session Static Mapping (Lifelong Mapping) LiDAR rig was mounted on a quadruped robot, and the same environment was mapped at appropriate time intervals. Subsequently, a mapping system was developed that can recognize dynamic and static objects and reflect changes in the environment.\n","date":1728000000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728000000,"objectID":"0631ffd5ecccaf45b8e506279901a556","permalink":"http://localhost:1313/post/24-10-04-timevariantmapping/","publishdate":"2024-10-04T00:00:00Z","relpermalink":"/post/24-10-04-timevariantmapping/","section":"post","summary":"“Development of SLAM Technology for Robots Adaptable to Temporal Changes”.\n","tags":null,"title":"LiDAR Robot Mapping","type":"post"},{"authors":["SNU Computer Vision Lab"],"categories":null,"content":"Development of cutting-edge deep learning technology for 3D reconstruction using only a few-shot of images.\nReconstructing high-quality 3D models from sparse visual data is a long-standing challenge in computer vision. In our latest work, we tackle this problem using a few-shot learning approach built on the recently popularized 3D Gaussian Splatting framework. Our goal was to develop an effective pipeline that could reconstruct faithful 3D representations of real-world objects — such as SNU souvenirs — from just a small set of input images.\nWe propose three main improvements over the baseline:\n1.\tDepth Loss: Using “Depth Anything” and Pearson correlation-based depth consistency improved SSIM while maintaining competitive perceptual quality (LPIPS).\n2.\tNormal Loss: Leveraging normals from StableNormal and enforcing self-consistency and unseen-view consistency enhanced fine geometric details and visual fidelity.\n3.\tGeometric Loss: A novel combination of flattening and expansion losses guided primitive shapes for more compact and accurate reconstructions.\nTogether, these additions improved the PSNR from 19.28 (baseline) to 20.02, and LPIPS from 0.448 to 0.371 under few-shot settings\nLive Demo\n","date":1720742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720742400,"objectID":"56f92a3f34081238616b694d08ee1cfa","permalink":"http://localhost:1313/post/24-08-09-snugift3d/","publishdate":"2024-07-12T00:00:00Z","relpermalink":"/post/24-08-09-snugift3d/","section":"post","summary":"Development of cutting-edge deep learning technology for 3D reconstruction using only a few-shot of images.\n","tags":null,"title":"Few-shot Reconstruction of SNU Souvenirs","type":"post"},{"authors":["SNU 3D Vision Lab."],"categories":null,"content":"Development of cutting-edge deep learning technology for 3D reconstruction using only a few-shot of images.\nCapturing complex architectural environments for immersive visualization is traditionally labor-intensive and computationally expensive. In this project, we present a fast, accessible, and high-quality 3D capture pipeline for Haedong Plaza, leveraging omnidirectional video and neural rendering techniques to enable realistic free-viewpoint navigation.\nWe employed NeRF-based rendering techniques to reconstruct and visualize the 3D scene: •\tProposal network sampling from Mip-NeRF 360 for efficient training in unbounded scenes. •\tPose refinement for accurate camera localization. •\tMulti-resolution hash grids from Instant-NGP for memory-efficient representation\nThe final output is a smooth and immersive rendering of Haedong Plaza from arbitrary viewpoints. The visualization faithfully captures complex geometry and lighting, demonstrating the pipeline’s viability for large indoor architectural spaces\n","date":1718150400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718150400,"objectID":"21c22660645103673e6e2d8dd0dd5cd1","permalink":"http://localhost:1313/post/24-08-09-indoor3d/","publishdate":"2024-06-12T00:00:00Z","relpermalink":"/post/24-08-09-indoor3d/","section":"post","summary":"Development of cutting-edge deep learning technology for 3D reconstruction using only a few-shot of images.\n","tags":null,"title":"Capturing \u0026 Free-viewpoint rendering of Haedong plaza","type":"post"},{"authors":["SNU Robot Vision Lab."],"categories":null,"content":"Development of LiDAR-Based SLAM Technology for Autonomous Navigation of Indoor and Outdoor Robots.\nAs part of the development of an indoor-outdoor autonomous robot, we built a LiDAR-based mapping system capable of capturing both indoor and outdoor environments, and conducted tests on the robot’s autonomous navigation.\n","date":1717286400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717286400,"objectID":"9a2e64a276649419ac533ddedfc350bf","permalink":"http://localhost:1313/post/24-12-02-robotnav/","publishdate":"2024-06-02T00:00:00Z","relpermalink":"/post/24-12-02-robotnav/","section":"post","summary":"Development of LiDAR-Based SLAM Technology for Autonomous Navigation of Indoor and Outdoor Robots.\n","tags":null,"title":"LiDAR SLAM based Outdoor-indoor robot navigation","type":"post"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"http://localhost:1313/contact/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"Contact","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"http://localhost:1313/people/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"landing"},{"authors":null,"categories":null,"content":"","date":1666569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666569600,"objectID":"b0d61e5cbb7472bf320bf0ef2aaeb977","permalink":"http://localhost:1313/tour/","publishdate":"2022-10-24T00:00:00Z","relpermalink":"/tour/","section":"","summary":"","tags":null,"title":"Tour","type":"landing"},{"authors":["SNU RVL Lab"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Add the publication’s full text or supplementary notes here. You can use rich formatting such as including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"http://localhost:1313/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":["SNU RVL Lab","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Add the publication’s full text or supplementary notes here. You can use rich formatting such as including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"http://localhost:1313/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["SNU RVL Lab","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Add the publication’s full text or supplementary notes here. You can use rich formatting such as including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"http://localhost:1313/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"}]